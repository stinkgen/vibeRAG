# VibeRAG Configuration

chat:
  model: ${CHAT_MODEL:-gpt-4}
  provider: ${CHAT_PROVIDER:-openai}
  temperature: ${CHAT_TEMPERATURE:-0.7}
  chunks_limit: ${CHAT_CHUNKS_LIMIT:-10}
  ollama_url: ${OLLAMA_HOST:-http://localhost:11434}

presentation:
  model: ${CHAT_MODEL:-gpt-4}
  provider: ${CHAT_PROVIDER:-openai}
  temperature: ${CHAT_TEMPERATURE:-0.7}
  chunks_limit: ${CHAT_CHUNKS_LIMIT:-10}

research:
  model: ${CHAT_MODEL:-gpt-4}
  provider: ${CHAT_PROVIDER:-openai}
  temperature: ${CHAT_TEMPERATURE:-0.5}
  chunks_limit: ${CHAT_CHUNKS_LIMIT:-10}

web_search:
  limit: 3

embedding:
  model_name: "all-MiniLM-L6-v2"
  batch_size: 32
  device: "cuda"  # Will fallback to CPU if CUDA not available

ingestion:
  chunk_size: 512  # Token limit per chunk
  overlap: 50      # Overlap between chunks

milvus:
  collection_name: ${MILVUS_COLLECTION:-vibe_chunks}
  embedding_dim: 384  # all-MiniLM-L6-v2's dimension
  default_batch_size: 100
  host: ${MILVUS_HOST:-localhost}
  port: ${MILVUS_PORT:-19530}
  index_params:
    metric_type: "L2"
    index_type: "IVF_FLAT"
    params:
      nlist: 1024
  search_params:
    nprobe: 16
  field_params:
    chunk_id:
      dtype: "INT64"
      is_primary: true
      auto_id: true
    doc_id:
      dtype: "VARCHAR"
      max_length: 128
    embedding:
      dtype: "FLOAT_VECTOR"
      dim: 384
    text:
      dtype: "VARCHAR"
      max_length: 65535
    metadata:
      dtype: "JSON"
    tags:
      dtype: "ARRAY"
      element_type: "VARCHAR"
      max_capacity: 50
      max_length: 128
    filename:
      dtype: "VARCHAR"
      max_length: 512

# Config's cleanâ€”magic strings are history! ðŸ”¥ 